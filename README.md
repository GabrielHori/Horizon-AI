<div align="center">

# ğŸš€ Horizon AI

### *One interface. Multiple intelligences.*

**A modern, secure, and high-performance desktop application for unified AI model interaction**

[![Built with Tauri](https://img.shields.io/badge/Built%20with-Tauri-24C8D8?style=for-the-badge&logo=tauri)](https://tauri.app)
[![React](https://img.shields.io/badge/React-18.x-61DAFB?style=for-the-badge&logo=react)](https://reactjs.org)
[![Python](https://img.shields.io/badge/Python-3.9+-3776AB?style=for-the-badge&logo=python)](https://python.org)
[![Rust](https://img.shields.io/badge/Rust-Latest-CE422B?style=for-the-badge&logo=rust)](https://rust-lang.org)

</div>

---

## ğŸ“– Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [Screenshots](#-screenshots)
- [Tech Stack](#-tech-stack)
- [Architecture](#-architecture)
- [Supported Providers](#-supported-providers)
- [Getting Started](#-getting-started)
- [Configuration](#-configuration)
- [Security](#-security)
- [Design Philosophy](#-design-philosophy)
- [Roadmap](#-roadmap)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸŒŸ Overview

**Horizon AI** is a cutting-edge desktop application that provides a **unified interface for interacting with multiple AI models**, both **local and cloud-based**. Built with a strong focus on **performance, modularity, security, and user control**, Horizon AI empowers you to leverage the best AI models available without being locked into a single provider.

The application is architected to be **extensible by design**, allowing seamless integration of new AI providers, models, and execution backends without disrupting the existing system.

### Why Horizon AI?

- ğŸ” **Privacy-First**: Run powerful AI models entirely offline on your local machine
- ğŸ¯ **Unified Experience**: One beautiful interface for all your AI needs
- âš¡ **High Performance**: Built with Rust (Tauri) for maximum speed and minimal resource usage
- ğŸ§© **Modular Design**: Easy to extend with new providers and features
- ğŸ”’ **Secure**: Encrypted data storage and sandboxed execution
- ğŸ’ **Beautiful UI**: Modern, responsive interface built with React and TailwindCSS

---

## âœ¨ Key Features

### ğŸ§  Multi-Provider AI Support
- **Ollama** - Local GGUF models for fast, private inference
- **AirLLM** - VRAM-optimized loading for large Hugging Face models
- **Cloud APIs** - Optional integration with Claude, OpenAI, and compatible services

### ğŸ” Intelligent Provider Routing
- Dynamic provider switching based on your needs
- Unified API across all providers
- Graceful fallback handling

### ğŸ–¥ï¸ Native Desktop Application
- Cross-platform support (Windows, macOS, Linux)
- Native performance with minimal memory footprint
- Tauri-based architecture for security and speed

### âš™ï¸ Granular Control
- Manual provider activation and deactivation
- Real-time status monitoring
- Resource usage visibility
- Model lifecycle management

### ğŸ“¦ Comprehensive Model Management
- Browse and download models
- Version control and updates
- Model metadata and performance stats
- Easy model switching

### ğŸ’¾ Persistent & Encrypted Storage
- Secure configuration management
- Encrypted chat history
- Project and memory persistence
- Custom encryption keys

### ğŸ”’ Offline-First Architecture
- Full functionality without internet connection
- Local model execution
- Privacy-preserving design

### ğŸ¨ Modern User Experience
- Intuitive, responsive interface
- Dark mode support
- Smooth animations and transitions
- Accessibility features

---

## ğŸ–¼ï¸ Screenshots

### ğŸ  Main Dashboard
The central hub for managing your AI interactions, with quick access to chat, models, and settings.

![Main Interface](screenshots/main-ui.png)

---

### ï¿½ Provider Selection
Easily switch between different AI providers with real-time status indicators.

![Provider Selection](screenshots/providers.png)

---

### ğŸ§  Model Manager (AirLLM)
Browse, download, and manage your AI models with detailed information and controls.

![AirLLM Model Selection](screenshots/airllm-models.png)

---

### â³ Model Loading
Real-time feedback during model initialization with progress indicators.

![Model Loading](screenshots/airllm-loading.png)

---

## ğŸ§± Tech Stack

### Frontend Layer
- **âš›ï¸ React 18** - Modern component-based UI framework
- **âš¡ Vite** - Next-generation frontend tooling for blazing-fast development
- **ğŸ¨ TailwindCSS** - Utility-first CSS framework for rapid UI development
- **ğŸ–¥ï¸ Tauri** - Rust-based framework for building secure, lightweight desktop applications

### Backend Layer
- **ğŸ¦€ Rust** - Core application logic and system integration via Tauri
- **ğŸ Python 3.9+** - AI model execution sidecar for AirLLM
- **ğŸ”Œ IPC Bridge** - Seamless communication between Rust and Python workers

### AI & Model Support
- **Ollama** - Local GGUF model inference engine
- **AirLLM** - Optimized Hugging Face model loader with efficient VRAM usage
- **Cloud APIs** - Integration with Claude, OpenAI, and OpenAI-compatible endpoints

### Data & Security
- **JSON Storage** - Lightweight, encrypted persistent storage
- **Cryptography** - AES encryption for sensitive data
- **Permission System** - Granular access control for system operations

---

## ğŸ—ï¸ Architecture

Horizon AI follows a modular, layered architecture that separates concerns and enables easy extensibility:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Frontend (React)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Dashboard â”‚  â”‚  Chat    â”‚  â”‚  Models  â”‚  â”‚ Settings â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Tauri IPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Backend (Rust/Tauri)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Command Handlers                        â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚   Provider  â”‚  â”‚   Model     â”‚  â”‚  Permission  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   Manager   â”‚  â”‚   Manager   â”‚  â”‚   Guard      â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama Service    â”‚  â”‚  Python Dispatcher   â”‚
â”‚  (Local/HTTP)      â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ AirLLM Worker  â”‚  â”‚
                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Encrypted Data  â”‚
                   â”‚   - Chat History â”‚
                   â”‚   - Projects     â”‚
                   â”‚   - Memory       â”‚
                   â”‚   - Config       â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Directory Structure

```
Horizon AI/
â”‚
â”œâ”€â”€ src/                         # Frontend React application
â”‚   â”œâ”€â”€ components/              # Reusable UI components
â”‚   â”œâ”€â”€ pages/                   # Page components (Dashboard, Chat, etc.)
â”‚   â”œâ”€â”€ services/                # Frontend services (API calls)
â”‚   â”œâ”€â”€ hooks/                   # Custom React hooks
â”‚   â”œâ”€â”€ utils/                   # Utility functions
â”‚   â””â”€â”€ styles/                  # Global styles
â”‚
â”œâ”€â”€ src-tauri/                   # Tauri backend (Rust)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.rs             # Application entry point
â”‚   â”‚   â”œâ”€â”€ permission_commands.rs  # Permission system
â”‚   â”‚   â”œâ”€â”€ model_manager.rs    # Model management
â”‚   â”‚   â””â”€â”€ ...                 # Other Rust modules
â”‚   â”œâ”€â”€ Cargo.toml              # Rust dependencies
â”‚   â””â”€â”€ tauri.conf.json         # Tauri configuration
â”‚
â”œâ”€â”€ python_dispatcher/           # Python sidecar
â”‚   â”œâ”€â”€ dispatcher.py           # Main dispatcher
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ollama_service.py   # Ollama integration
â”‚   â”‚   â”œâ”€â”€ airllm_service.py   # AirLLM integration
â”‚   â”‚   â”œâ”€â”€ chat_history_service.py
â”‚   â”‚   â”œâ”€â”€ memory_service.py
â”‚   â”‚   â””â”€â”€ project_service.py
â”‚   â””â”€â”€ helpers/
â”‚       â””â”€â”€ file_storage_helper.py  # Encrypted file operations
â”‚
â”œâ”€â”€ screenshots/                 # Application screenshots
â”‚
â””â”€â”€ README.md                   # This file
```

---

## ğŸ§  Supported Providers

### âœ… Ollama (Local GGUF Models)

**Best for**: Day-to-day AI tasks, privacy-focused usage, fast responses

- âœ¨ **Fast Startup** - Models load in seconds
- ğŸ”’ **100% Offline** - No internet required
- âš¡ **Low Latency** - Near-instant responses
- ğŸ’ª **Wide Model Support** - Llama, Mistral, Phi, and more
- ğŸ¯ **Easy Setup** - Install Ollama and you're ready

**Supported Models**: Any GGUF model compatible with Ollama

---

### âœ… AirLLM (Optimized Hugging Face Models)

**Best for**: Running large models with limited VRAM, experimental models

- ğŸ§  **VRAM-Efficient** - Load models larger than your GPU memory
- ğŸ›ï¸ **Manual Control** - Start/stop on demand
- ğŸ“Š **Resource Monitoring** - Real-time VRAM and RAM usage
- ğŸ” **GPU Safety** - Single-instance execution prevents conflicts
- ğŸ¨ **Model Selection UI** - Browse and select from Hugging Face

**Supported Models**: Most Hugging Face transformer models

---

### â˜ï¸ Cloud Providers (Optional)

**Best for**: Access to cutting-edge models, no local hardware required

- ğŸŒ **Claude** - Anthropic's powerful language models
- ğŸ¤– **OpenAI** - GPT models via official API
- ğŸ”Œ **OpenAI-Compatible** - Any service with OpenAI-compatible endpoints

**Note**: Requires API keys and internet connection

---

## âš™ï¸ Provider Lifecycle

Each provider in Horizon AI follows a well-defined lifecycle for predictable behavior and resource management:

### States

- `OFF` - Provider is disabled and not using resources
- `LOADING` - Provider is initializing or loading a model
- `READY` - Provider is active and ready to process requests
- `ERROR` - Provider encountered an issue

### Operations

- **Enable/Disable** - Control whether a provider is available
- **Load/Unload** - Manage resource allocation
- **Select** - Choose which provider handles requests
- **Monitor** - View status and resource usage in real-time

### State Transitions

```
OFF â†’ [User enables] â†’ LOADING â†’ [Success] â†’ READY
                               â†’ [Failure] â†’ ERROR

READY â†’ [User disables] â†’ OFF
READY â†’ [Error occurs] â†’ ERROR
ERROR â†’ [User retries] â†’ LOADING
```

---

## ï¿½ Getting Started

### Prerequisites

Before installing Horizon AI, ensure you have:

- **Node.js** (v16 or higher) - [Download](https://nodejs.org/)
- **Rust** (latest stable) - [Install](https://rustup.rs/)
- **Python 3.9+** - [Download](https://python.org/)
- **Ollama** (for local models) - [Install](https://ollama.ai/)
- **CUDA-compatible GPU** (recommended for AirLLM, optional)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/GabrielHori/Horizon-AI.git
   cd Horizon-AI
   ```

2. **Install Node dependencies**
   ```bash
   npm install
   ```

3. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure Tauri**
   
   The Tauri configuration is located in `src-tauri/tauri.conf.json`. Review and adjust settings as needed.

### Development

Run the application in development mode with hot-reload:

```bash
npm run dev
```

This will:
- Start the Vite development server for the React frontend
- Launch the Tauri application window
- Enable hot-module replacement for rapid development

### Building for Production

Create optimized production builds:

```bash
# Build for your current platform
npm run build

# The executable will be in src-tauri/target/release/
```

For platform-specific builds:

```bash
# Windows
npm run build -- --target x86_64-pc-windows-msvc

# macOS
npm run build -- --target x86_64-apple-darwin

# Linux
npm run build -- --target x86_64-unknown-linux-gnu
```

---

## ğŸ”§ Configuration

### Application Settings

Configuration is stored in encrypted JSON files within the application data directory:

- **Windows**: `%APPDATA%\horizon-ai\`
- **macOS**: `~/Library/Application Support/horizon-ai/`
- **Linux**: `~/.config/horizon-ai/`

### Environment Variables

Create a `.env` file in the project root for optional settings:

```env
# Python Dispatcher
PYTHON_DISPATCHER_PORT=8000

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434

# Optional: Cloud API Keys
CLAUDE_API_KEY=your_key_here
OPENAI_API_KEY=your_key_here
```

### Encryption

Horizon AI uses AES encryption for sensitive data. On first launch, you'll be prompted to set an encryption password. This password is used to:

- Encrypt chat history
- Protect project data
- Secure memory storage
- Safeguard configuration

**Important**: Keep your encryption password safe. Data cannot be recovered without it.

---

## ï¿½ Security

Horizon AI is built with security as a core principle:

### Data Protection
- ğŸ” **AES Encryption** - All sensitive data encrypted at rest
- ğŸ”‘ **Unique Salt** - Each installation uses a unique cryptographic salt
- ğŸ›¡ï¸ **Sandboxed Execution** - Tauri's security model isolates the application

### Permission System
- âœ… **Granular Controls** - Fine-grained permissions for file access and system operations
- ğŸš« **Least Privilege** - Only request necessary permissions
- ğŸ“ **Audit Trail** - Track permission requests and grants

### Content Security Policy (CSP)
- ğŸŒ **Strict CSP** - Prevents XSS and injection attacks
- ğŸ”’ **HTTPS Only** - Secure external communications

### Best Practices
- ğŸ”„ **Regular Updates** - Keep dependencies up to date
- ğŸ§ª **Code Reviews** - All changes reviewed for security implications
- ğŸ“Š **Static Analysis** - Automated security scanning

---

## ğŸ§  Design Philosophy

Horizon AI is built on core principles that guide every design decision:

### ğŸ¯ Explicit User Control
- No surprises or hidden behavior
- Clear, actionable controls for all operations
- User decides when and how resources are used

### ğŸš« No Forced Providers
- Mix and match providers based on your needs
- No vendor lock-in
- Freedom to go fully offline or cloud-based

### ğŸ” Transparency
- No hidden background services
- Visible resource usage
- Clear status indicators

### ğŸ“Š Clear Model Lifecycle
- Predictable state management
- Obvious transitions
- Error states clearly communicated

### ğŸ§© Clean Architecture
- Separation of concerns
- Modular design
- Loose coupling, high cohesion

### ğŸ”Œ Extensibility
- Add new providers without refactoring
- Plugin-ready architecture (coming soon)
- Well-documented APIs for integration

---

## ğŸ›£ï¸ Roadmap

### Version 2.1.0 (In Progress)
- [ ] ğŸ’¬ Streaming responses for real-time feedback
- [ ] ğŸ¨ Visual Prompt Builder UI
- [ ] ğŸš€ Performance optimizations
- [ ] ğŸ§ª Comprehensive E2E tests

### Version 2.2.0 (Planned)
- [ ] ğŸ”€ Advanced routing rules (fallback chains, load balancing)
- [ ] ğŸ§  Long-term conversation memory
- [ ] ğŸ“Š Model benchmarking and comparison tools
- [ ] ğŸ­ Multiple personas/agents

### Version 3.0.0 (Future)
- [ ] ğŸ”Œ Plugin system for community extensions
- [ ] ğŸŒ Multi-language support (UI localization)
- [ ] ğŸ“± Mobile companion app
- [ ] ğŸ¤ Collaborative features (shared chats, team workspaces)

### Additional Providers (Ongoing)
- [ ] Google Gemini integration
- [ ] Cohere support
- [ ] Local Stable Diffusion for image generation
- [ ] Whisper for speech-to-text

---

## ğŸ¤ Contributing

We welcome contributions from the community! Here's how you can help:

### Code Contributions

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/amazing-feature`)
3. **Make your changes**
4. **Test thoroughly**
5. **Commit with clear messages** (`git commit -m 'Add amazing feature'`)
6. **Push to your fork** (`git push origin feature/amazing-feature`)
7. **Open a Pull Request**

### Guidelines

- âœ… **Keep changes modular** - One feature/fix per PR
- ğŸ”’ **Don't break existing providers** - Ensure backwards compatibility
- ğŸ“ **Follow project structure** - Place files in appropriate directories
- ğŸ“ **Document new features** - Update README and add inline comments
- ğŸ§ª **Add tests** - Include unit and integration tests for new code
- ğŸ¨ **Match code style** - Use existing formatting conventions

### Reporting Issues

Found a bug or have a feature request? Please open an issue:

1. Check existing issues to avoid duplicates
2. Use a clear, descriptive title
3. Provide detailed reproduction steps (for bugs)
4. Include system information (OS, Node version, etc.)
5. Add screenshots if applicable

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ§‘â€ğŸ’» Author

**Gabriel (Horizon)**

- GitHub: [@GabrielHori](https://github.com/GabrielHori)
- Project: [Horizon AI](https://github.com/GabrielHori/Horizon-AI)

---

## ğŸ™ Acknowledgments

Special thanks to:

- **Tauri Team** - For the amazing desktop framework
- **Ollama** - For making local AI accessible
- **Hugging Face** - For the incredible model ecosystem
- **Open Source Community** - For all the amazing tools and libraries

---

<div align="center">

### ğŸŒŸ Star this repo if you find it useful!

**Horizon AI** â€” *One interface. Multiple intelligences.*

Made with â¤ï¸ and â˜•

</div>
