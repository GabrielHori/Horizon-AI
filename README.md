# ðŸš€ Horizon AI

**Horizon AI** is a modern desktop application that provides a **unified interface for multiple AI models**, both **local and cloud-based**, with a strong focus on **performance, modularity, and user control**.

The project is built to be **extensible by design**, allowing new AI providers, models, and execution backends to be added without breaking the existing architecture.

---

## âœ¨ Key Features

- ðŸ§  **Multi-provider AI support**
  - Ollama (local GGUF models)
  - AirLLM (VRAM-optimized Hugging Face models)
  - Cloud APIs (Claude, OpenAI â€“ optional)
- ðŸ” **Unified provider router**
- ðŸ–¥ï¸ **Desktop application**
- âš™ï¸ **Manual provider activation**
- ðŸ“¦ **Model selection & lifecycle management**
- ðŸš¦ **Real-time provider status**
- ðŸ§© **Modular & scalable architecture**
- ðŸ’¾ **Persistent configuration**
- ðŸ”’ **Offline-first** with local models

---

## ðŸ§± Tech Stack

### Frontend
- âš›ï¸ React
- âš¡ Vite
- ðŸŽ¨ TailwindCSS
- ðŸ–¥ï¸ Tauri

### Backend
- ðŸ§µ Worker-based backend (Node / TypeScript)
- ðŸ”Œ Provider routing system
- ðŸ Python sidecar (AirLLM)

### AI / Providers
- Ollama â†’ Local GGUF models
- AirLLM â†’ Hugging Face models with optimized VRAM usage
- Cloud APIs â†’ Claude / OpenAI (optional)

---

## ðŸ—ï¸ Project Architecture

```
Horizon AI
â”‚
â”œâ”€â”€ frontend/               # React + Tailwind UI
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ worker/             # Core backend worker
â”‚   â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ managers/
â”‚   â”‚   â””â”€â”€ router/
â”‚   â”‚
â”‚   â””â”€â”€ sidecars/
â”‚       â””â”€â”€ airllm_sidecar.py
â”‚
â”œâ”€â”€ screenshots/            # Application screenshots
â”‚
â””â”€â”€ config.json             # Persistent configuration
```

---

## ðŸ§  Supported Providers

### âœ… Ollama
- Local GGUF models
- Fast startup
- Low latency
- Ideal for daily usage

### âœ… AirLLM
- Manually enabled
- VRAM-efficient loading
- User-selected models
- Start / stop on demand
- Single-instance execution (GPU safety)

### â˜ï¸ Cloud Providers (Optional)
- Claude
- OpenAI
- Any OpenAI-compatible API

---

## âš™ï¸ Provider Lifecycle

Each provider can be:
- Enabled / Disabled
- Loaded / Unloaded
- Selected at runtime

### Provider States
- `OFF`
- `LOADING`
- `READY`
- `ERROR`

---

## ðŸ–¥ï¸ Application Screenshots

> Place your screenshots inside the `screenshots/` folder at the project root.

### ðŸ  Main Interface
```md
![Main Interface](screenshots/main-ui.png)
```

### ðŸ”Œ Provider Selection
```md
![Provider Selection](screenshots/providers.png)
```

### ðŸ§  Model Selection (AirLLM)
```md
![AirLLM Model Selection](screenshots/airllm-models.png)
```

### â³ Model Loading
```md
![Model Loading](screenshots/airllm-loading.png)
```

### ðŸ’¬ Chat Interface
```md
![Chat Interface](screenshots/chat.png)
```

---

## ðŸš€ Getting Started

### Prerequisites
- Node.js
- Python 3.9+
- Ollama installed
- CUDA-compatible GPU recommended (for AirLLM)

### Install dependencies
```bash
npm install
```

### Run in development
```bash
npm run dev
```

### Build desktop application
```bash
npm run build
```

---

## ðŸ§  Design Philosophy

- Explicit user control
- No forced providers
- No hidden background services
- Clear model lifecycle
- Clean separation of concerns
- Extensible without refactoring

---

## ðŸ›£ï¸ Roadmap

- Streaming responses
- Advanced routing rules
- Conversation memory
- Plugin system
- Model benchmarking
- Additional providers

---

## ðŸ¤ Contributing

Contributions are welcome.

- Keep changes modular
- Do not break existing providers
- Follow existing project structure
- Document new providers

---

## ðŸ§‘â€ðŸ’» Author

**Gabriel (Horizon)**

---

> Horizon AI â€” *One interface. Multiple intelligences.*
