mod python_bridge;
mod ollama_installer;

use python_bridge::PythonBridge;
use tauri::{Manager, Wry, AppHandle, RunEvent};
use serde_json::Value;
use std::process::Command;

#[cfg(windows)]
use std::os::windows::process::CommandExt;

// Flag pour masquer la fenÃªtre CMD sur Windows
#[cfg(windows)]
const CREATE_NO_WINDOW: u32 = 0x08000000;

#[tauri::command]
async fn call_python(
    state: tauri::State<'_, PythonBridge<Wry>>, 
    cmd: String,
    payload: Value
) -> Result<Value, String> {
    state.send(cmd, payload).await
}

/// VÃ©rifie si Ollama est installÃ©
#[tauri::command]
fn check_ollama_installed() -> bool {
    ollama_installer::is_ollama_installed()
}

/// Installe Ollama automatiquement
#[tauri::command]
async fn install_ollama(app: AppHandle<Wry>) -> Result<(), String> {
    ollama_installer::download_and_install_ollama(&app).await
}

/// DÃ©marre le service Ollama
#[tauri::command]
fn start_ollama() -> Result<(), String> {
    ollama_installer::start_ollama_service()
}

// ========================================
// COMMANDES DE FENÃŠTRE PERSONNALISÃ‰ES
// ========================================

/// Minimise la fenÃªtre
#[tauri::command]
async fn minimize_window(window: tauri::Window) -> Result<(), String> {
    window.minimize().map_err(|e| e.to_string())
}

/// Maximise ou restaure la fenÃªtre
#[tauri::command]
async fn toggle_maximize(window: tauri::Window) -> Result<bool, String> {
    if window.is_maximized().unwrap_or(false) {
        window.unmaximize().map_err(|e| e.to_string())?;
        Ok(false)
    } else {
        window.maximize().map_err(|e| e.to_string())?;
        Ok(true)
    }
}

/// Ferme la fenÃªtre (et l'application)
#[tauri::command]
async fn close_window(window: tauri::Window) -> Result<(), String> {
    window.close().map_err(|e| e.to_string())
}

/// VÃ©rifie si la fenÃªtre est maximisÃ©e
#[tauri::command]
fn is_maximized(window: tauri::Window) -> bool {
    window.is_maximized().unwrap_or(false)
}

/// ArrÃªte le processus Ollama
fn stop_ollama() {
    #[cfg(windows)]
    {
        // Sur Windows, utiliser taskkill pour arrÃªter ollama.exe silencieusement
        let _ = Command::new("taskkill")
            .args(["/F", "/IM", "ollama.exe"])
            .creation_flags(CREATE_NO_WINDOW)
            .output();
        
        // Aussi arrÃªter ollama_llama_server si prÃ©sent
        let _ = Command::new("taskkill")
            .args(["/F", "/IM", "ollama_llama_server.exe"])
            .creation_flags(CREATE_NO_WINDOW)
            .output();
    }
    
    #[cfg(not(windows))]
    {
        let _ = Command::new("pkill")
            .arg("ollama")
            .output();
    }
    
    #[cfg(debug_assertions)]
    println!("ðŸ›‘ Ollama: Service arrÃªtÃ©");
}

#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub fn run() {
    let app = tauri::Builder::default()
        .plugin(tauri_plugin_shell::init())
        .plugin(tauri_plugin_dialog::init())
        .plugin(tauri_plugin_fs::init()) 
        .plugin(tauri_plugin_log::Builder::new().build())
        .setup(|app| {
            // Initialisation unique du Bridge Python
            let bridge = PythonBridge::<Wry>::new(&app.handle());
            app.manage(bridge); 
            
            // âœ… DÃ‰MARRER OLLAMA AU LANCEMENT (si installÃ©)
            if ollama_installer::is_ollama_installed() {
                #[cfg(debug_assertions)]
                println!("ðŸš€ Ollama: DÃ©marrage automatique...");
                
                let _ = ollama_installer::start_ollama_service();
            }
            
            #[cfg(debug_assertions)]
            println!("ðŸš€ Horizon AI: Application dÃ©marrÃ©e");
            
            Ok(())
        })
        .invoke_handler(tauri::generate_handler![
            call_python,
            check_ollama_installed,
            install_ollama,
            start_ollama,
            minimize_window,
            toggle_maximize,
            close_window,
            is_maximized
        ])
        .build(tauri::generate_context!())
        .expect("Erreur lors du build de l'application Horizon AI");
    
    // âœ… GESTION DES Ã‰VÃ‰NEMENTS DE FERMETURE
    app.run(|_app_handle, event| {
        match event {
            RunEvent::ExitRequested { .. } | RunEvent::Exit => {
                #[cfg(debug_assertions)]
                println!("ðŸ›‘ Horizon AI: Fermeture en cours...");
                
                // ArrÃªter Ollama proprement Ã  la fermeture
                stop_ollama();
            }
            _ => {}
        }
    });
}
