mod python_bridge;
mod ollama_installer;

use python_bridge::PythonBridge;
use tauri::{Manager, Wry, AppHandle};
use serde_json::Value;

#[tauri::command]
async fn call_python(
    state: tauri::State<'_, PythonBridge<Wry>>, 
    cmd: String,
    payload: Value
) -> Result<Value, String> {
    state.send(cmd, payload).await
}

/// V√©rifie si Ollama est install√©
#[tauri::command]
fn check_ollama_installed() -> bool {
    ollama_installer::is_ollama_installed()
}

/// Installe Ollama automatiquement
#[tauri::command]
async fn install_ollama(app: AppHandle<Wry>) -> Result<(), String> {
    ollama_installer::download_and_install_ollama(&app).await
}

/// D√©marre le service Ollama
#[tauri::command]
fn start_ollama() -> Result<(), String> {
    ollama_installer::start_ollama_service()
}

#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub fn run() {
    tauri::Builder::default()
        .plugin(tauri_plugin_shell::init())
        .plugin(tauri_plugin_dialog::init())
        .plugin(tauri_plugin_fs::init()) 
        .plugin(tauri_plugin_log::Builder::new().build())
        .setup(|app| {
            // --- LOGIQUE DE L'AUDIT ---
            // On ne lance plus le sidecar ici manuellement avec .sidecar("backend").spawn()
            // car cela cr√©e un premier processus Python orphelin.
            // C'est le PythonBridge::new() qui va s'en charger de mani√®re encapsul√©e.

            // Initialisation unique du Bridge
            // Le bridge va d√©marrer le sidecar une seule fois √† l'int√©rieur de son constructeur
            let bridge = PythonBridge::<Wry>::new(&app.handle());
            
            // On enregistre l'√©tat pour que call_python puisse y acc√©der
            app.manage(bridge); 
            
            #[cfg(debug_assertions)]
            println!("üöÄ Horizon AI: Syst√®me de communication initialis√© (Single Instance)");
            
            Ok(())
        })
        .invoke_handler(tauri::generate_handler![
            call_python,
            check_ollama_installed,
            install_ollama,
            start_ollama
        ])
        .run(tauri::generate_context!())
        .expect("Erreur lors du lancement de l'application Horizon AI");
}
